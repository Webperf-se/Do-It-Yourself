{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 02 - Samla adresser"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\" \nCrawling websites for URLs\n\"\"\"\nfrom urllib.request import urlopen, Request\nimport requests\nfrom bs4 import BeautifulSoup\nimport datetime\nimport random\nimport re\nimport time\n\nsec_timeout = 90*1   # 1 minute from now\nwhile_timeout = time.time() + sec_timeout\nrandom.seed(datetime.datetime.now())\n\ndef get_links(site, pageUrl):\n    try:\n        html = urlopen(Request('{0}{1}'.format(site, pageUrl), headers={'User-Agent': 'Webperf.se Crawler'}))\n        bs = BeautifulSoup(html, 'html.parser')\n\n        return bs.find_all('a', href=re.compile('^/')) # only URLs starting with /\n    except Exception as e:\n        print(site, pageUrl, '\\n', e)\n        return None\n\ndef check_for_redirect(url):\n    try:\n        r = requests.head(url, allow_redirects=True, timeout = 5)\n        return r.url, r.history, r.status_code, r.headers['Content-Type']\n    except:\n        return None\n\ndef harvest_links(site = 'https://www.vgregion.se', initial_page = '/', max_pages = 50):\n    links = get_links(site, initial_page)\n    i = 0\n    urls = []\n\n    print('Looking for at most {} URLs at {} for {} seconds'.format(max_pages, site, sec_timeout))\n\n    while len(links) > 0 and i < max_pages:\n        newPage = links[random.randint(0, len(links)-1)].attrs['href']\n        \n        if site + newPage not in urls and 'mailto' not in newPage and '#' not in newPage and newPage != None and 'http' not in newPage and '.pdf' not in newPage and '.docx' not in newPage and '.pptx' not in newPage:\n            \n            check_redir = check_for_redirect(site + newPage)\n            if(check_redir is not None and site in check_redir[0] and check_redir[2] == 200 and 'text/html' in check_redir[3]):\n                i += 1\n                print(i, newPage)\n                urls.append(site + newPage)\n                \n                new_links = get_links(site, newPage)\n                if new_links is not None:\n                    links = new_links\n            else:\n                print('Redirection, content-type or status code error detected. URL skipped.\\n', check_redir) # check_redir[0] do not work for all content-types\n        \n        if time.time() > while_timeout:\n            break\n    \n    return urls\n\nharvest_links(site='https://www.vgregion.se', max_pages=25)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}